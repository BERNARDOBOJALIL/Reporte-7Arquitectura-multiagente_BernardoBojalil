# Sistema Multiagente

## Datos generales del alumno

**Nombre:** Bernardo Bojalil Lorenzini
**Matr√≠cula:** 195908 
**Curso:** Agentes Inteligentes  
**Per√≠odo:** Oto√±o 2025  


## Introducci√≥n

Este proyecto implementa un sistema multiagente colaborativo dise√±ado para automatizar la creaci√≥n de art√≠culos t√©cnicos sobre tecnolog√≠a. El sistema est√° basado en una arquitectura horizontal donde tres agentes especializados (Investigador, Redactor y Editor) trabajan de manera coordinada mediante un sistema de mensajer√≠a descentralizado.

El objetivo principal es demostrar c√≥mo m√∫ltiples agentes con roles espec√≠ficos pueden colaborar eficientemente para completar una tarea compleja. Cada agente utiliza el modelo Gemini 2.5 Flash de Google para procesar informaci√≥n y generar contenido de calidad profesional. El sistema simula un flujo de trabajo editorial real, desde la investigaci√≥n inicial hasta la publicaci√≥n del art√≠culo final.

## Desarrollo de la soluci√≥n

### Sistema de mensajer√≠a entre agentes

El n√∫cleo de la comunicaci√≥n entre agentes se implementa mediante dos clases fundamentales que permiten el intercambio descentralizado de informaci√≥n:

```python
class MensajeAgente:
    """Representa un mensaje entre agentes"""
    def __init__(self, remitente: str, destinatario: str, contenido: str, tipo: str = "datos"):
        self.remitente = remitente
        self.destinatario = destinatario
        self.contenido = contenido
        self.tipo = tipo
        self.timestamp = datetime.now().isoformat()
    
    def to_dict(self):
        return {
            "remitente": self.remitente,
            "destinatario": self.destinatario,
            "tipo": self.tipo,
            "timestamp": self.timestamp
        }

class BuzonMensajes:
    """Sistema de mensajer√≠a descentralizado para comunicaci√≥n entre agentes"""
    def __init__(self):
        self.mensajes: List[MensajeAgente] = []
        self.historial: List[Dict] = []
    
    def enviar_mensaje(self, remitente: str, destinatario: str, contenido: str, tipo: str = "datos"):
        mensaje = MensajeAgente(remitente, destinatario, contenido, tipo)
        self.mensajes.append(mensaje)
        self.historial.append(mensaje.to_dict())
        print(f"\nüì® {remitente} ‚Üí {destinatario}")
        return mensaje
    
    def obtener_mensajes(self, destinatario: str) -> List[MensajeAgente]:
        mensajes_pendientes = [m for m in self.mensajes if m.destinatario == destinatario]
        self.mensajes = [m for m in self.mensajes if m.destinatario != destinatario]
        return mensajes_pendientes
    
    def guardar_historial(self, ruta: str = "historial_mensajes.json"):
        with open(ruta, "w", encoding="utf-8") as f:
            json.dump(self.historial, f, ensure_ascii=False, indent=2)
```

La clase `MensajeAgente` encapsula toda la informaci√≥n necesaria para la comunicaci√≥n entre agentes. Cada mensaje contiene cuatro atributos principales: el nombre del agente que lo env√≠a (remitente), el agente que debe recibirlo (destinatario), el contenido del mensaje (que puede ser texto extenso como datos de investigaci√≥n o borradores completos), y un tipo que permite categorizar los mensajes (como "investigacion" o "borrador"). Adicionalmente, cada mensaje registra autom√°ticamente la fecha y hora exacta en que fue creado mediante `timestamp`, lo cual es crucial para auditor√≠as posteriores. El m√©todo `to_dict()` convierte el mensaje a un diccionario que puede ser serializado a JSON para su almacenamiento permanente.

La clase `BuzonMensajes` act√∫a como el canal de comunicaci√≥n central del sistema. Mantiene dos estructuras de datos: una lista de mensajes activos que a√∫n no han sido procesados por sus destinatarios, y un historial completo de todos los mensajes que se han enviado durante la ejecuci√≥n. El m√©todo `enviar_mensaje()` crea un nuevo objeto `MensajeAgente`, lo a√±ade a la cola de mensajes pendientes y simult√°neamente lo registra en el historial permanente. Adem√°s, imprime en consola una notificaci√≥n visual del flujo de informaci√≥n entre agentes. El m√©todo `obtener_mensajes()` implementa un patr√≥n de consumo de mensajes: busca todos los mensajes dirigidos a un agente espec√≠fico, los extrae de la cola de pendientes (elimin√°ndolos para evitar procesamiento duplicado), y los devuelve al agente solicitante. Finalmente, `guardar_historial()` persiste todo el registro de comunicaciones en un archivo JSON, permitiendo an√°lisis posterior del flujo de trabajo completo.

### Clase base para agentes

Todos los agentes del sistema heredan de una clase base que estandariza su comportamiento y proporciona la infraestructura com√∫n necesaria:

```python
class AgenteBase:
    """Clase base para todos los agentes del sistema"""
    def __init__(self, nombre: str, rol: str, buzon: BuzonMensajes, temperature: float = 0.3):
        self.nombre = nombre
        self.rol = rol
        self.buzon = buzon
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=temperature,
            google_api_key=google_key,
        )
    
    def log(self, mensaje: str):
        print(f"[{self.nombre}] {mensaje}")
    
    def enviar_mensaje(self, destinatario: str, contenido: str, tipo: str = "datos"):
        return self.buzon.enviar_mensaje(self.nombre, destinatario, contenido, tipo)
    
    def recibir_mensajes(self) -> List[MensajeAgente]:
        return self.buzon.obtener_mensajes(self.nombre)
```

El constructor de `AgenteBase` inicializa cuatro componentes esenciales para cada agente. Primero, asigna un nombre √∫nico y una descripci√≥n de rol que identifican al agente en el sistema. Segundo, guarda una referencia al buz√≥n de mensajes compartido, estableciendo la conexi√≥n del agente con el sistema de comunicaci√≥n. Tercero, y m√°s importante, crea una instancia del modelo de lenguaje Gemini 2.5 Flash mediante LangChain, configur√°ndolo con un par√°metro `temperature` que controla la creatividad vs. determinismo de las respuestas del modelo. Una temperatura m√°s baja produce respuestas m√°s consistentes y predecibles, mientras que valores m√°s altos generan respuestas m√°s variadas y creativas.

El m√©todo `log()` proporciona una forma estandarizada de imprimir mensajes en consola, anteponiendo el nombre del agente entre corchetes para facilitar el seguimiento de qu√© agente est√° ejecutando cada acci√≥n. Los m√©todos `enviar_mensaje()` y `recibir_mensajes()` son wrappers convenientes que abstraen la interacci√≥n con el buz√≥n de mensajes: el primero autom√°ticamente incluye el nombre del agente como remitente, mientras que el segundo solicita mensajes dirigidos espec√≠ficamente a ese agente. Esta abstracci√≥n permite que cada agente especializado se enfoque en su l√≥gica de negocio sin preocuparse por los detalles de la comunicaci√≥n.

### Agente Investigador

El primer agente del flujo de trabajo se especializa en la recopilaci√≥n y estructuraci√≥n de informaci√≥n sobre temas tecnol√≥gicos:

```python
class ResearchAgent(AgenteBase):
    def __init__(self, buzon: BuzonMensajes):
        super().__init__("Agente Investigador", "Experto en b√∫squeda de informaci√≥n", buzon, temperature=0.3)
        self.prompt_template = PromptTemplate(
            input_variables=["tema"],
            template="""
Eres un investigador de tecnolog√≠a. Busca informaci√≥n objetiva y actual sobre el tema solicitado.

TEMA: {tema}

Proporciona:
1. Definici√≥n y contexto del tema (2 p√°rrafos)
2. Datos y estad√≠sticas relevantes (3-5 puntos)
3. Aplicaciones pr√°cticas actuales
4. Tendencias y desarrollos recientes
5. Empresas o proyectos destacados

S√© objetivo, conciso y preciso. Enf√≥cate en hechos verificables.
"""
        )
    
    def investigar(self, tema: str) -> str:
        self.log(f"Investigando: {tema}")
        chain = self.prompt_template | self.llm | StrOutputParser()
        datos_encontrados = chain.invoke({"tema": tema})
        self.enviar_mensaje("Agente Redactor", datos_encontrados, tipo="investigacion")
        return datos_encontrados
```

El `ResearchAgent` hereda de `AgenteBase` y se inicializa con un nombre descriptivo, un rol que define su expertise, y una temperatura de 0.3 relativamente baja. Esta temperatura conservadora es intencional: queremos que el agente produzca informaci√≥n factual y consistente, no respuestas creativas o especulativas. El coraz√≥n del agente es su `prompt_template`, un objeto de LangChain que define c√≥mo se comunica con el modelo de lenguaje. Este template utiliza una variable de entrada `{tema}` que ser√° reemplazada din√°micamente con el tema espec√≠fico que se desea investigar. El prompt est√° cuidadosamente estructurado en cinco secciones espec√≠ficas, solicitando al LLM que act√∫e como un investigador profesional que recopila definiciones, datos cuantificables, aplicaciones reales, tendencias emergentes y actores relevantes en el campo. La instrucci√≥n final enfatiza objetividad y precisi√≥n, orientando al modelo hacia respuestas basadas en hechos.

El m√©todo `investigar()` ejecuta el proceso de investigaci√≥n completo. Primero registra en consola qu√© tema est√° siendo investigado mediante el m√©todo `log()`. Luego construye una cadena de procesamiento usando el operador pipe de LangChain: el template genera el prompt completo, este se env√≠a al modelo de lenguaje (`self.llm`), y finalmente el `StrOutputParser()` extrae la respuesta como texto plano. Al invocar esta cadena con el diccionario `{"tema": tema}`, el placeholder en el template se reemplaza con el tema real y el LLM genera la investigaci√≥n. Los datos obtenidos se env√≠an inmediatamente al siguiente agente en el flujo (Agente Redactor) mediante el sistema de mensajer√≠a, marc√°ndolos con el tipo "investigacion" para que el receptor sepa c√≥mo procesarlos. Finalmente, el m√©todo retorna los datos para permitir validaciones o usos adicionales.

### Agente Redactor

El segundo agente transforma la informaci√≥n estructurada en un art√≠culo narrativo bien escrito:

```python
class WriterAgent(AgenteBase):
    def __init__(self, buzon: BuzonMensajes):
        super().__init__("Agente Redactor", "Experto en redacci√≥n t√©cnica", buzon, temperature=0.4)
        self.prompt_template = PromptTemplate(
            input_variables=["datos_investigacion"],
            template="""
Eres un redactor t√©cnico. Escribe un art√≠culo de blog claro y objetivo basado en los datos proporcionados.

DATOS DE INVESTIGACI√ìN:
{datos_investigacion}

ESTRUCTURA:
1. T√≠tulo descriptivo
2. Introducci√≥n (2 p√°rrafos): qu√© es y por qu√© es importante
3. Secci√≥n 1: Fundamentos y contexto
4. Secci√≥n 2: Aplicaciones y casos de uso
5. Secci√≥n 3: Estado actual y tendencias
6. Conclusi√≥n: resumen y perspectiva

ESTILO:
- Tono profesional e informativo
- P√°rrafos cortos y directos
- Enfoque en hechos y datos
- Evita lenguaje promocional o especulativo
- 700-900 palabras

Genera el borrador completo del art√≠culo.
"""
        )
    
    def redactar(self, datos: str) -> str:
        self.log("Redactando art√≠culo...")
        chain = self.prompt_template | self.llm | StrOutputParser()
        borrador_articulo = chain.invoke({"datos_investigacion": datos})
        self.enviar_mensaje("Agente Editor", borrador_articulo, tipo="borrador")
        return borrador_articulo
    
    def procesar_mensajes(self):
        mensajes = self.recibir_mensajes()
        for mensaje in mensajes:
            if mensaje.tipo == "investigacion":
                return self.redactar(mensaje.contenido)
        return None
```

El `WriterAgent` se inicializa con una temperatura de 0.4, ligeramente m√°s alta que el investigador. Este ajuste es estrat√©gico: queremos mantener precisi√≥n factual pero permitir algo m√°s de flexibilidad ling√º√≠stica y creatividad en la estructura narrativa del art√≠culo. El prompt template est√° dise√±ado para recibir los datos de investigaci√≥n completos y transformarlos en un art√≠culo estructurado. Especifica una arquitectura clara de seis secciones que van desde un t√≠tulo atractivo hasta una conclusi√≥n sint√©tica. Las instrucciones de estilo son detalladas: establecen el tono profesional, proh√≠ben expl√≠citamente el lenguaje promocional o especulativo (com√∫n en blogs de tecnolog√≠a de baja calidad), y definen un rango de longitud objetivo de 700-900 palabras que equilibra profundidad con legibilidad.

El m√©todo `redactar()` implementa la l√≥gica de transformaci√≥n. Registra su actividad en consola, construye la cadena de procesamiento LangChain id√©ntica a la del investigador (pero con un template diferente), invoca el LLM pas√°ndole los datos de investigaci√≥n, y recibe un borrador completo del art√≠culo. Este borrador se env√≠a inmediatamente al Agente Editor marcado con el tipo "borrador", continuando el flujo del pipeline.

El m√©todo `procesar_mensajes()` implementa un patr√≥n cr√≠tico en arquitecturas de mensajer√≠a: el consumo selectivo de mensajes. Este m√©todo recupera todos los mensajes pendientes para el agente, itera sobre ellos, y procesa √∫nicamente aquellos marcados como "investigacion" (ignorando cualquier otro tipo). Esto hace que el agente sea robusto: si por alguna raz√≥n recibe mensajes de tipos inesperados, simplemente los ignora. Al encontrar un mensaje de investigaci√≥n, llama al m√©todo `redactar()` con el contenido del mensaje y retorna el resultado. Si no hay mensajes de investigaci√≥n pendientes, retorna `None`, indicando que no hay trabajo por hacer.

### Agente Editor

El tercer agente realiza la revisi√≥n final y produce el art√≠culo listo para publicaci√≥n:

```python
class EditorAgent(AgenteBase):
    def __init__(self, buzon: BuzonMensajes):
        super().__init__("Agente Editor", "Experto en revisi√≥n y edici√≥n", buzon, temperature=0.2)
        self.prompt_template = PromptTemplate(
            input_variables=["borrador"],
            template="""
Eres un editor profesional. Revisa y mejora el borrador para producir la versi√≥n final.

BORRADOR:
{borrador}

TAREAS:
1. Corrige errores ortogr√°ficos y gramaticales
2. Mejora la claridad y coherencia
3. Verifica la estructura y flujo
4. Optimiza t√≠tulos y subt√≠tulos
5. Asegura tono profesional consistente

IMPORTANTE: Genera √öNICAMENTE el art√≠culo final corregido y mejorado, sin comentarios adicionales.
No incluyas notas, res√∫menes de cambios ni sugerencias. Solo el art√≠culo listo para publicar.
"""
        )
    
    def revisar(self, borrador: str) -> str:
        self.log("Revisando y generando versi√≥n final...")
        chain = self.prompt_template | self.llm | StrOutputParser()
        articulo_final = chain.invoke({"borrador": borrador})
        print("\n‚úÖ ¬°TAREA COMPLETADA! Art√≠culo Finalizado.")
        return articulo_final
    
    def procesar_mensajes(self):
        mensajes = self.recibir_mensajes()
        for mensaje in mensajes:
            if mensaje.tipo == "borrador":
                return self.revisar(mensaje.contenido)
        return None
```

El `EditorAgent` utiliza la temperatura m√°s baja del sistema (0.2), maximizando la determinismo y consistencia en las correcciones. Un editor debe aplicar reglas gramaticales y de estilo de manera predecible, no experimentar con variaciones creativas. El prompt template est√° optimizado para revisi√≥n editorial profesional, enumerando cinco tareas espec√≠ficas que van desde correcciones ortogr√°ficas b√°sicas hasta verificaci√≥n de coherencia estructural. La secci√≥n "IMPORTANTE" es crucial: instruye expl√≠citamente al LLM para que NO genere metacontenido como "He corregido los siguientes errores..." o "Sugerencias de mejora...". Esto es necesario porque los LLMs tienden naturalmente a explicar sus acciones; aqu√≠ queremos √∫nicamente el producto final pulido.

El m√©todo `revisar()` sigue el patr√≥n establecido: registra su actividad, construye la cadena LangChain con el template de edici√≥n, procesa el borrador recibido, y genera el art√≠culo final. Al completar, imprime un mensaje de √©xito con el emoji de checkmark verde, proporcionando feedback visual claro de que el pipeline completo ha terminado exitosamente. El m√©todo `procesar_mensajes()` replica la l√≥gica del Agente Redactor pero filtrando por mensajes tipo "borrador", manteniendo el patr√≥n de consumo selectivo que hace robusto todo el sistema.

### Coordinador del sistema

La clase coordinadora orquesta la ejecuci√≥n secuencial del pipeline completo y gestiona las salidas del sistema:

```python
class CoordinadorMultiagente:
    def __init__(self):
        self.buzon = BuzonMensajes()
        self.agente_investigador = ResearchAgent(self.buzon)
        self.agente_redactor = WriterAgent(self.buzon)
        self.agente_editor = EditorAgent(self.buzon)
    
    def generar_articulo(self, tema: str) -> str:
        print("\n" + "="*60)
        print(f"SISTEMA MULTIAGENTE - BLOG DE TECNOLOG√çA")
        print("="*60)
        print(f"Tema: {tema}\n")
        
        # FASE 1: Investigaci√≥n
        print("FASE 1: INVESTIGACI√ìN")
        print("-"*60)
        datos = self.agente_investigador.investigar(tema)
        
        # FASE 2: Redacci√≥n
        print("\nFASE 2: REDACCI√ìN")
        print("-"*60)
        self.agente_redactor.procesar_mensajes()
        
        # FASE 3: Edici√≥n
        print("\nFASE 3: REVISI√ìN Y EDICI√ìN")
        print("-"*60)
        articulo_final = self.agente_editor.procesar_mensajes()
        
        if articulo_final:
            self._guardar_articulo(tema, articulo_final)
            self.buzon.guardar_historial()
            return articulo_final
        else:
            raise Exception("Error: No se pudo completar el art√≠culo")
    
    def _guardar_articulo(self, tema: str, articulo: str):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        nombre_archivo = f"articulo_{timestamp}.txt"
        
        with open(nombre_archivo, "w", encoding="utf-8") as f:
            f.write(f"TEMA: {tema}\n")
            f.write(f"FECHA: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n\n")
            f.write(articulo)
        
        print(f"\nüíæ Art√≠culo guardado: {nombre_archivo}")
```

El constructor del `CoordinadorMultiagente` inicializa todo el ecosistema de agentes. Primero crea una instancia √∫nica del buz√≥n de mensajes que ser√° compartida por todos los agentes, estableciendo el canal de comunicaci√≥n com√∫n. Luego instancia cada uno de los tres agentes especializados pas√°ndoles la referencia al buz√≥n compartido. Este patr√≥n de "inyecci√≥n de dependencias" es fundamental: todos los agentes comparten el mismo buz√≥n, lo que permite que los mensajes fluyan entre ellos de manera transparente.

El m√©todo `generar_articulo()` implementa el flujo de trabajo completo dividido en tres fases claramente delimitadas. Comienza imprimiendo un encabezado visual con el tema solicitado para proporcionar contexto claro en la consola. La Fase 1 activa directamente al agente investigador mediante su m√©todo `investigar()`, que internamente enviar√° sus resultados al redactor v√≠a mensajer√≠a. La Fase 2 no llama a un m√©todo del redactor directamente, sino que invoca `procesar_mensajes()`, lo cual es sutilmente importante: el redactor busca activamente mensajes de tipo "investigacion" en su buz√≥n, los encuentra (porque el investigador los envi√≥ en la fase anterior), y procesa el contenido. Este patr√≥n desacopla a los agentes: el coordinador no necesita pasar datos expl√≠citamente entre ellos. La Fase 3 replica este patr√≥n con el editor procesando mensajes tipo "borrador".

Despu√©s de las tres fases, el m√©todo verifica que se haya generado un art√≠culo final exitosamente. Si existe, llama a `_guardar_articulo()` para persistir el resultado y a `guardar_historial()` del buz√≥n para registrar toda la comunicaci√≥n entre agentes. Si algo fall√≥ y no hay art√≠culo final, lanza una excepci√≥n expl√≠cita. El m√©todo privado `_guardar_articulo()` genera un nombre de archivo √∫nico usando timestamp, escribe el art√≠culo con metadatos de tema y fecha, y notifica al usuario d√≥nde se guard√≥ el archivo.

## Pruebas

El sistema fue probado exhaustivamente con m√∫ltiples temas tecnol√≥gicos para validar su robustez y calidad de salida. Para ejecutarlo correctamente:

1. Configurar la API key de Google en archivo `.env`:
```
GOOGLE_API_KEY=tu_api_key_aqui
```

2. Instalar dependencias necesarias:
```bash
pip install langchain-google-genai python-dotenv
```

3. Ejecutar el sistema en modo interactivo:
```bash
python sistema_multiagente.py
```

Se realizaron pruebas con diversos temas:
### Prueba 1: Relojes Mec√°nicos

Se ejecut√≥ el sistema con el tema "Relojes Mec√°nicos" para evaluar su capacidad de generar contenido t√©cnico sobre un tema no estrictamente digital.

**Entrada del usuario:**
```
Tema: Relojes Mec√°nicos
```

![Ejecuci√≥n del sistema mostrando las tres fases del pipeline](imagenes/prueba1_consola.png)
*Figura 1: Consola mostrando la ejecuci√≥n de las fases de Investigaci√≥n, Redacci√≥n y Edici√≥n*

![Art√≠culo final generado sobre relojes mec√°nicos](imagenes/prueba1_salida.png)
*Figura 2: Art√≠culo final generado y guardado en archivo de texto*

### Prueba 2: Atlixco

Se prob√≥ el sistema con el tema "Atlixco" para validar su comportamiento con temas no tecnol√≥gicos y evaluar la capacidad de los agentes para adaptarse a contextos diferentes.

**Entrada del usuario:**
```
Tema: Atlixco
```

![Ejecuci√≥n del sistema procesando el tema Atlixco](imagenes/prueba2_consola.png)
*Figura 3: Consola mostrando el procesamiento del tema Atlixco a trav√©s de las tres fases*

![Art√≠culo generado sobre Atlixco](imagenes/prueba2_salida.png)
*Figura 4: Resultado final del art√≠culo sobre Atlixco*

---

En todas las pruebas, el sistema complet√≥ exitosamente las tres fases del pipeline, generando art√≠culos coherentes de 700-900 palabras con estructura profesional. Los tiempos de ejecuci√≥n variaron entre 45-90 segundos dependiendo de la complejidad del tema. Los archivos generados incluyen marcas temporales √∫nicas (formato `articulo_YYYYMMDD_HHMMSS.txt`) y el sistema genera un archivo `historial_mensajes.json` que registra cada mensaje intercambiado entre agentes, permitiendo auditor√≠as detalladas del proceso.


## Conclusiones

Este proyecto demuestra c√≥mo un sistema multiagente puede trabajar para generar contenido, simulando la din√°mica de un peque√±o equipo de trabajo. Tres agentes especializados, el investigador, el redactor y el editor colaboran de forma horizontal, aportando cada uno su propia ‚Äúforma de pensar‚Äù y sin depender de jerarqu√≠as ni supervisi√≥n central. Esta estructura permite que cada componente se enfoque en su tarea: el investigador recopila datos confiables, el redactor transforma la informaci√≥n en un texto claro y atractivo, y el editor pule el estilo final. Al dividir el trabajo de esta manera, el sistema produce resultados m√°s consistentes y coherentes que si un solo modelo intentara hacerlo todo.

La arquitectura horizontal del proyecto facilita la colaboraci√≥n, y tambi√©n ofrece una gran flexibilidad y facilidad de mantenimiento. Al no existir un agente ‚Äújefe‚Äù, cada componente puede modificarse, reemplazarse o ampliarse sin afectar al resto del sistema. Adem√°s, la comunicaci√≥n entre agentes es estructurada y transparente: cada mensaje incluye metadatos y marcas de tiempo, lo que permite auditar el flujo de trabajo, identificar cuellos de botella y optimizar la generaci√≥n de contenido. Esto convierte al sistema en un generador de art√≠culos y una plataforma que se puede analizar y mejorar con datos reales.

La integraci√≥n de los modelos de lenguaje mediante LangChain y el uso de Gemini 2.5 Flash hicieron posible manejar cadenas de tareas, prompts y resultados de manera √°gil. Gracias a su dise√±o modular, el sistema est√° pensado para crecer: se podr√≠an a√±adir f√°cilmente agentes adicionales sin alterar la base existente. En conjunto, el proyecto muestra c√≥mo la cooperaci√≥n entre agentes especializados permite generar contenido de manera m√°s inteligente y eficiente.